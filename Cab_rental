rm(list=ls(all=T))
setwd("C:/Users/hp/Desktop/date time finel copy r")

#Load Libraries
x = c("ggplot2", "corrgram", "DMwR", "caret", "randomForest", "unbalanced", "C50", "dummies", "e1071", "Information",
      "MASS", "rpart", "gbm", "ROSE", 'sampling', 'DataCombine', 'inTrees')

install.packages(x)
lapply(x, require, character.only = TRUE)
rm(x)

## Read the data
fare = read.csv("train_cab.csv", header = F, na.strings = c(" ", "", "NA"))

###########################################Explore the data##########################################
str(fare)

library(stringr)
#str_split_fixed(fare$pickup_datetime, "UTC", 2)
library(tidyr)

#now remove extra pickup datetime
fare=  subset(fare, select = -pickup_datetime)
#separate pickup_datetime from UTC and form datetime column
fare = separate(fare, col= pickup_datetime, into="datetime", sep="UTC" , remove =T )

#find the percentage of missing value
missing_val = data.frame(apply(fare,2,function(x){sum(is.na(x))}))
missing_val$Columns = row.names(missing_val)
names(missing_val)[1] =  "Missing_percentage"
missing_val$Missing_percentage = (missing_val$Missing_percentage/nrow()) * 100
missing_val = missing_val[order(-missing_val$Missing_percentage),]
row.names(missing_val) = NULL
missing_val = missing_val[,c(2,1)]
write.csv(missing_val, "Miising_perc.csv", row.names = F)
# ggplot(data = missing_val[1:3,], aes(x=reorder(Columns, -Missing_percentage),y = Missing_percentage))+
#   geom_bar(stat = "identity",fill = "grey")+xlab("Parameter")+
#   ggtitle("Missing data percentage (Train)") + theme_bw()

#Mean Method
sapply(fare, class)
)  
# for fare amount =16.9
#mean value=15.014
#median value=8.5
#knn value =7.9130
# SO HERE WE CHOOSE MEAN FOR FARE AMOUNT

# for passenger count=3
#mean value =2.625
#median value=1
#mode value=1
#knn value =2.44
# HERE WE CHOOSE mode VALUE FOR PASSENGER COUNT BECAUSE PASSENGER COUNT CAN NOT COMES IN DECIMAL 

# for date =(2014, 11, 12)
#mode value=2011-06-13
#knn=2014-11-12 

#for time(9, 9, 21)
#mode value=19:43:00 
#knn=(9, 9, 25)
# so for fare amount nearest value is from mean and for passenger count nearest value is from knn
# DATE AND TIME HAS ONLY ONE MISSING VALUE WE CAN IGNORE IT OR IMPUTE BY ANY METHOD


#numeric_index = sapply(fare,is.numeric) #selecting only numeric
fare$fare_amount = as.numeric(fare_amount)
#put mean at the place of nan
fare$fare_amount[is.na(fare$fare_amount)] = mean(fare$fare_amount, na.rm = T)
fare$passenger_count=fare[passenger_count].fillna(1, inplace=True)


# kNN Imputation
fare = knnImputation(fare, k = 3)
#drop datetime nan
fare= drop_na(fare)
##Data Manupulation; convert string categories into factor numeric
for(i in 1:ncol(fare)){
  
  if(class(fare[,i]) == 'factor'){
    
    fare[,i] = factor(fare[,i], labels=(1:length(levels(factor(fare[,i])))))
    
  }
}


############################################Outlier Analysis#############################################
# ## BoxPlots - Distribution and Outlier Check
numeric_index = sapply(fare,is.numeric) #selecting only numeric

numeric_data = fare[,numeric_index]

cnames = colnames(numeric_data)
# 
for (i in 1:length(cnames))
{
  assign(paste0("gn",i), ggplot(aes_string(y = (cnames[i]), x = "responded"), data = subset(fare))+ 
           stat_boxplot(geom = "errorbar", width = 0.5) +
           geom_boxplot(outlier.colour="red", fill = "grey" ,outlier.shape=18,
                        outlier.size=1, notch=FALSE) +
           theme(legend.position="bottom")+
           labs(y=cnames[i],x="responded")+
           ggtitle(paste("Box plot of responded for",cnames[i])))
}
# 
# ## Plotting plots together
# gridExtra::grid.arrange(gn1,gn5,gn2,ncol=3)
# gridExtra::grid.arrange(gn6,gn7,ncol=2)
# gridExtra::grid.arrange(gn8,gn9,ncol=2)
# 
# # #Remove outliers using boxplot method
# df = fare
# fare = df
# 
# val = marketing_train$previous[marketing_train$previous %in% boxplot.stats(marketing_train$previous)$out]
# 
# marketing_train = marketing_train[which(!marketing_train$previous %in% val),]
#                                   
# # #loop to remove from all variables
for(i in cnames){
  print(i)
  val = fare[,i][fare[,i] %in% boxplot.stats(fare[,i])$out]
  #print(length(val))
  fare = fare[which(!fare[,i] %in% val),]
}
# 
# #Replace all outliers with NA and impute
# #create NA on "custAge
# for(i in cnames){
#   val = fare[,i][fare[,i] %in% boxplot.stats(fare[,i])$out]
#   #print(length(val))
#   fare[,i][fare[,i] %in% val] = NA
# }
# 
# fare = knnImputation(marketing_train, k = 3)

##################################Feature Selection################################################
## Correlation Plot 
corrgram(fare[,numeric_index], order = F,
         upper.panel=panel.pie, text.panel=panel.txt, main = "Correlation Plot")

## Chi-squared Test of Independence
factor_index = sapply(fare,is.factor)
factor_data = fare[,factor_index]

for (i in 1:8)
{
  print(names(factor_data)[i])
  print(chisq.test(table(factor_data$responded,factor_data[,i])))
}

## Dimension Reduction
marketing_train = subset(fare, 
                         select = -c(pdays,emp.var.rate,day_of_week, loan, housing))

##################################Feature Scaling################################################
#Normality check
qqnorm(fare$fare_amount)
hist(fare$fare_amount )

#Normalisation
cnames = c('dropoff_latitude','fare_amount', 'pickup_longitude', 'pickup_latitude', 'passenger_count',"dropoff_longitude)

for(i in cnames){
  print(i)
  fare[,i] = (fare[,i] - min(fare[,i]))/
       (max(fare[,i] - min(fare[,i])))
}

# #Standardisation
# for(i in cnames){
#   print(i)
#   fare[,i] = (fare[,i] - mean(fare[,i]))/
#                                  sd(fare[,i])
# }

#############################################Sampling#############################################
# ##Simple Random Sampling
# data_sample = fare[sample(nrow(fare), 4000, replace = F), ]
# 
# ##Stratified Sampling
# stratas = strata(fare, c("profession"), size = c(100, 199, 10, 5), method = "srswor")
# stratified_data = getdata(fare, stratas)
# 
# ##Systematic sampling
# #Function to generate Kth index
# sys.sample = function(N,n){
#   k = ceiling(N/n)
#   r = sample(1:k, 1)
#   sys.samp = seq(r, r + k*(n-1), k)
# }
# 
# lis = sys.sample(7414, 400) #select the repective rows
# 
# #Create index variable in the data
# marketing_train$index = 1:7414
# 
# #Extract subset from whole data
# systematic_data = marketing_train[which(marketing_train$index %in% lis),]

###################################Model Development#######################################
#Clean the environment
rmExcept("marketing_train")

#Divide data into train and test using stratified sampling method
set.seed(1234)
train.index = createDataPartition(fare$fare_amount, p = .80, list = FALSE)
train = fare[ train.index,]
test  = fare[-train.index,]

##Decision tree for regression
#Develop Model on training data
C50_model = C5.0(responded ~., train, trials = 100, rules = TRUE)

#Summary of DT model
summary(C50_model)

#write rules into disk
write(capture.output(summary(C50_model)), "c50Rules.txt")

#Lets predict for test cases
C50_Predictions = predict(C50_model, test[,-17], type = "class")

##Evaluate the performance of regression  model
rowMeans(abs((actual-C50_predictIONS)/actual) * 100)

#

#ERROR=28.29

###Random Forest
RF_model = randomForest(responded ~ ., train, importance = TRUE, ntree = 500)

#Extract rules fromn random forest
#transform rf object to an inTrees' format
# treeList = RF2List(RF_model)  
# 
# #Extract rules
# exec = extractRules(treeList, train[,-17])  # R-executable conditions
# 
# #Visualize some rules
# exec[1:2,]
# 
# #Make rules more readable:
# readableRules = presentRules(exec, colnames(train))
# readableRules[1:2,]
 

# 
# #evaulate few rules
# ruleMetric[1:2,]

#Presdict test data using random forest model
RF_Predictions = predict(RF_model, test[,-17])

##Evaluate the performance BU MAPE

#MAPE=18.58

#Linear Regression
linear_model = glm(responded ~ ., data = train, family = "binomial")

#summary of the model
summary(linear_model)

#predict using logistic regression
linear_Predictions = predict(linear_model, newdata = test, type = "response")


rowMeans(abs((actual-linear_predictions)/actual) * 100)



#MAPE=41.58

##KNN Implementation
library(class)

#Predict test data
KNN_Predictions = knn(train[, 1:8], test[, 1:8], train$fare_amount, k = 7)

rowMeans(abs((actual-linear_predictions)/actual) * 100)




#mape error = 24.56%


